{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TFIDF_from_scratch_demo.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMcR0/ap1BzMVZFyGPVY116"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"itQGwsKQcsip","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1596877425886,"user_tz":-420,"elapsed":722,"user":{"displayName":"hieu nguyen ngoc","photoUrl":"","userId":"11515232851802008635"}},"outputId":"c0104e48-0b03-488f-d2ca-308f51ff5e54"},"source":["import nltk\n","import string\n","import math\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","dataset=[\n","         \"This is the first sentence. Hope you enjoy it !\",\n","        \"I love reading this book  .\",\n","        \"That is the second sentence.\",\n","        \"I hate my dog and my sister love her. Tommorow I will kill my dog and buy a new cat.\"\n","]\n","\n","punc_list=string.punctuation\n","for i in range(len(dataset)):\n","  dataset[i]=dataset[i].lower()\n","  tokens=word_tokenize(dataset[i])\n","  for token in tokens:\n","    token=token.lower()\n","    if token in punc_list:\n","      dataset[i]=dataset[i].replace(token,\"\")\n","      dataset[i]=dataset[i].replace(\" \",\" \")\n","\n","#calculate TF\n","def TF_at_rows(dataset,index):\n","  tf_dict_at_t={}\n","  sent_length=len(dataset[index].split())\n","  for word in dataset[index].split():\n","    if word in tf_dict_at_t:\n","      tf_dict_at_t[word]+=1\n","    else:\n","      tf_dict_at_t[word]=1\n","  \n","  for word in tf_dict_at_t:\n","    tf_dict_at_t[word]/=sent_length\n","  \n","  return tf_dict_at_t\n","\n","def TF(dataset):\n","  tf_map=[]\n","  for i in range(len(dataset)):\n","    tf_map.append(TF_at_rows(dataset,i))\n","  return tf_map\n","\n","tf_map=TF(dataset)\n","\n","#calculate frequency for IDF\n","def count_on_dataset(tf_map,dataset):\n","  count_on_ds={}\n","  for sample in tf_map:\n","    for word in sample:\n","      if word in count_on_ds:\n","        count_on_ds[word]+=1\n","      else:\n","        count_on_ds[word]=1\n","  return count_on_ds\n","#calculate IDF\n","def IDF(dataset,freq_on_dataset):\n","  idf_map={}\n","  for word in freq_on_dataset:\n","    if(freq_on_dataset[word]==0):\n","      idf_map[word]=math.log(len(dataset)/(1))\n","    else:\n","      idf_map[word]=math.log(len(dataset)/(freq_on_dataset[word]))\n","  return idf_map\n","\n","\n","freq_on_dataset=count_on_dataset(tf_map,dataset)\n","print(freq_on_dataset)\n","idf_map=IDF(dataset,freq_on_dataset)\n","print(\"IDF: \\n\",idf_map)\n","\n","#calculate TFIDF\n","def TFIDF(tf_map,idf_map):\n","  tfidf_map=[]\n","  for sample in tf_map:\n","    temp_tfidf={}\n","    for word in sample:\n","      temp_tfidf[word]=sample[word]*idf_map[word]\n","    tfidf_map.append(temp_tfidf)\n","  return tfidf_map\n","\n","print(\"TFIDF at first sample : \\n\",TFIDF(tf_map,idf_map)[0])"],"execution_count":96,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","{'this': 2, 'is': 2, 'the': 2, 'first': 1, 'sentence': 2, 'hope': 1, 'you': 1, 'enjoy': 1, 'it': 1, 'i': 2, 'love': 2, 'reading': 1, 'book': 1, 'that': 1, 'second': 1, 'hate': 1, 'my': 1, 'dog': 1, 'and': 1, 'sister': 1, 'her': 1, 'tommorow': 1, 'will': 1, 'kill': 1, 'buy': 1, 'a': 1, 'new': 1, 'cat': 1}\n","IDF: \n"," {'this': 0.6931471805599453, 'is': 0.6931471805599453, 'the': 0.6931471805599453, 'first': 1.3862943611198906, 'sentence': 0.6931471805599453, 'hope': 1.3862943611198906, 'you': 1.3862943611198906, 'enjoy': 1.3862943611198906, 'it': 1.3862943611198906, 'i': 0.6931471805599453, 'love': 0.6931471805599453, 'reading': 1.3862943611198906, 'book': 1.3862943611198906, 'that': 1.3862943611198906, 'second': 1.3862943611198906, 'hate': 1.3862943611198906, 'my': 1.3862943611198906, 'dog': 1.3862943611198906, 'and': 1.3862943611198906, 'sister': 1.3862943611198906, 'her': 1.3862943611198906, 'tommorow': 1.3862943611198906, 'will': 1.3862943611198906, 'kill': 1.3862943611198906, 'buy': 1.3862943611198906, 'a': 1.3862943611198906, 'new': 1.3862943611198906, 'cat': 1.3862943611198906}\n","TFIDF at first sample : \n"," {'this': 0.07701635339554948, 'is': 0.07701635339554948, 'the': 0.07701635339554948, 'first': 0.15403270679109896, 'sentence': 0.07701635339554948, 'hope': 0.15403270679109896, 'you': 0.15403270679109896, 'enjoy': 0.15403270679109896, 'it': 0.15403270679109896}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kBz8M9s5dhXb","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}