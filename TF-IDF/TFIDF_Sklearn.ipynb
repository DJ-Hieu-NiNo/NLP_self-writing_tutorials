{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFIDF_Sklearn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZx3Tsfo6pl-",
        "colab_type": "text"
      },
      "source": [
        "# **Read data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBdZmknQ4NHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d93a8f5c-40cf-4f26-e845-d733f35f3d0b"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(smooth_idf=True,ngram_range=(1,1),stop_words='english')\n",
        "dataset=[\n",
        "         \"This is the first sentence. Hope you enjoy it !\",\n",
        "        \"I love reading this book  .\",\n",
        "        \"That is the second sentence.\",\n",
        "        \"I hate my dog and my sister love her. Tommorow I will kill my dog and buy a new cat.\"\n",
        "]\n",
        "\n",
        "tfidf = vectorizer.fit_transform(corpus).toarray()\n",
        "feature_names=vectorizer.get_feature_names()\n",
        "print(\"Vocabs : \",feature_names)\n",
        "print(\"Vocab size : \",len(feature_names))\n",
        "print(\"TFIDF matrix has shape : \",tfidf.shape)\n",
        "print(tfidf)\n",
        "print(tfidf[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabs :  ['book', 'buy', 'cat', 'dog', 'enjoy', 'hate', 'hope', 'kill', 'love', 'new', 'reading', 'second', 'sentence', 'sister', 'tommorow']\n",
            "Vocab size :  15\n",
            "TFIDF matrix has shape :  (4, 15)\n",
            "[[0.         0.         0.         0.         0.61761437 0.\n",
            "  0.61761437 0.         0.         0.         0.         0.\n",
            "  0.48693426 0.         0.        ]\n",
            " [0.61761437 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.48693426 0.         0.61761437 0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.78528828\n",
            "  0.6191303  0.         0.        ]\n",
            " [0.         0.29333722 0.29333722 0.58667444 0.         0.29333722\n",
            "  0.         0.29333722 0.23127044 0.29333722 0.         0.\n",
            "  0.         0.29333722 0.29333722]]\n",
            "[0.         0.         0.         0.         0.61761437 0.\n",
            " 0.61761437 0.         0.         0.         0.         0.\n",
            " 0.48693426 0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HhcVd068M6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "cd83a8dc-c3a1-4f08-f8d6-a4143ab043a7"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "countvectorizer=CountVectorizer(lowercase=True,stop_words='english')\n",
        "word_count_vector=countvectorizer.fit_transform(dataset)\n",
        "print(\"Vocab : \",countvectorizer.get_feature_names())\n",
        "print(word_count_vector.toarray())\n",
        "\n",
        "tf_idf_transformer=TfidfTransformer()\n",
        "tfidf=tf_idf_transformer.fit_transform(word_count_vector).toarray()\n",
        "print(\"tfidf map : \\n\",tfidf)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab :  ['book', 'buy', 'cat', 'dog', 'enjoy', 'hate', 'hope', 'kill', 'love', 'new', 'reading', 'second', 'sentence', 'sister', 'tommorow']\n",
            "[[0 0 0 0 1 0 1 0 0 0 0 0 1 0 0]\n",
            " [1 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            " [0 1 1 2 0 1 0 1 1 1 0 0 0 1 1]]\n",
            "tfidf map : \n",
            " [[0.         0.         0.         0.         0.61761437 0.\n",
            "  0.61761437 0.         0.         0.         0.         0.\n",
            "  0.48693426 0.         0.        ]\n",
            " [0.61761437 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.48693426 0.         0.61761437 0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.78528828\n",
            "  0.6191303  0.         0.        ]\n",
            " [0.         0.29333722 0.29333722 0.58667444 0.         0.29333722\n",
            "  0.         0.29333722 0.23127044 0.29333722 0.         0.\n",
            "  0.         0.29333722 0.29333722]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRaiKyyHcSia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}